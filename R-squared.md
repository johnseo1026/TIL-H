## R-squared 결정계수

> R-squared는 우리말로는 `결정계수`라고 하며, R2로도 표현한다.
> 회귀모형의 설득력을 표현하는 것. P-value와 같이 0과 1 사이의 값으로 나타나는데, 0에 가까울수록 설득력이 낮고, 1에 가까울수록 높다고 해석할 수 있다.

![스크린샷 2020-04-14 오전 4.40.42](/Users/baesohyun/Library/Application Support/typora-user-images/스크린샷 2020-04-14 오전 4.40.42.jpg)

>  회귀분석은 위처럼 흩어진 데이터들을 가장 잘 설명하는 하나의 선(y^)을 구하는 방법. 그 선이 얼마나 데이터를 잘 설명할 수 있는가에 대한 점수가 R-squared이다.  SST와 SSR이 얼마나 비슷한지, SST와 SSE가 얼마나 다른지에 따라 R-squared 값이 높아질 수 있다. 
>
> 

![스크린샷 2020-04-14 오후 4.30.23](image/스크린샷 2020-04-14 오후 4.30.23.jpg)



>   `SST`는 `Sum of Square Total`로 편차의 제곱합이다. 
>
> 식: SST=∑ni=1(yi−^yi+^yi−¯y)2, SST=∑i=1n(yi−yi^+yi^−y¯)2.  모든 차이를 제곱하여 더한 것이다. 실제값과 예측값, 평균값 사이에 발생한 차이이며, 이것이 SSE와 SSR이다. SST=SSE+SSR .

>  `SSE`는 `Sum of Square Error`로, 회귀식과 실제값의 차이를 의미. 위의 그림을 다시 확인해보면, 파란선 y^ 와 검은점 사이에 격차가 존재한다. 이 유격을 제곱해 더한 것이 SSE다. 
>
> 식: SSE=∑(yi−^yi)2SSE=∑(yi−y^i)2 . 이 거리가 작을수록 y^ 이 모든 데이터를 고르게 설명한다고 해석할 수 있고, 높은 R-squared 값을 도출할 수 있다.

> `SSR`은 `Sum of Square Regression`으로, 회귀식과 평균값의 차이를 의미한다. 위의 그림에서 파란선 y^ 과 빨간선 y¯ 이 엇갈리며 차이가 발생하는 것을 확인할 수 있다. 이것의 식은  SSR=∑(^yi−¯y)2SSR=∑(y^i−y¯)2 . 평균 y¯ 와 차이가 날수록 SSR의 값이 커지는데, 이는 y^ 가 모든 데이터를 고루 설명하고 있다는 것으로 해석할 수 있다. SSR이 높아질수록 R-squared가 높아진다.

 ex) 중고차의 가격예측 예시. 자동차의 연식을 독립변수로 가격을 예측한다고 했을 때, 연식은 독립변수, 가격은 종속변수가 될 것이고, 위의 그림처럼 우상향하는 그래프를 얻을 수 있을 것이다.

이때 SSR값이 높게 나온다면? 연식을 기반으로 한 파란선 y^ 이 각 데이터를 고르게 설명한다는 의미가 되며, 해당 회귀식이 믿을 만 하다고 해석할 수 있을 것이다.

 반대로 SSE가 높다면?  y^ 이 각 데이터를 제대로 해석하지 못하고 있으며, 연식 말고 다른 변수가 가격에 영향을 주고 있다고 해석할 수 있다.

>  R-squared의 함정: 학습이 된 모델은 기존 데이터에 대해선 높은 R-squared 값을 가지게 될 것이다. 기존에 없던 새로운 옵션이 추가된 모델이라면 이 모델은 과적합이 발생하게 되며 새로운 데이터를 받았을 때 정확도를 보장할 수 없게 된다.

>  R-squared 장점: 학습데이터에 대한 모델의 신뢰도를 쉽게 파악할 수 있다.

>   P-value처럼 편리하지만, 해석에 주의가 필요한 개념인 것이다. R-squared는 연구에 사용된 독립변수가 얼마나 적합한지 판단할 때 사용하여야 하고, 학습된 모델의 적합성을 F검정을 비롯한 다른 방법을 통해 얻어야 한다.



[
](https://jihongl.github.io/2017/09/16/Rsquared/03.jpg)