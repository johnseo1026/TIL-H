{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib  # pip install nibabel\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # pip install tqdm\n",
    "import h5py   # pip install h5py\n",
    "import json\n",
    "\n",
    "import sys; sys.argv=['']; del sys #Make argsparser work in Jupyter notebooks\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Convert Decathlon raw Nifti data (http://medicaldecathlon.com) files to Numpy data files\",\n",
    "    add_help=True, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument(\"--data_path\",default=\"data/decathlon/Task01_BrainTumour/\",help=\"Path to the raw BraTS datafiles\")\n",
    "parser.add_argument(\"--save_path\",default=\"data/decathlon/\",help=\"Folder to save Numpy data files\")\n",
    "parser.add_argument(\"--output_filename\",default=\"Task01_BrainTumour.h5\",help=\"Name of the output HDF5 file\")\n",
    "parser.add_argument(\"--resize\", type=int, default=144, help=\"Resize height and width to this size. Original size = 240\")\n",
    "parser.add_argument(\"--split\", type=float, default=0.85, help=\"Train/test split ratio\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, cropx, cropy, cropz):\n",
    "    \"\"\"\n",
    "    Take a center crop of the images.\n",
    "    If we are using a 2D model, then we'll just stack the\n",
    "    z dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, z, c = img.shape\n",
    "\n",
    "    # Make sure starting index is >= 0\n",
    "    startx = max(x // 2 - (cropx // 2), 0)\n",
    "    starty = max(y // 2 - (cropy // 2), 0)\n",
    "    startz = max(z // 2 - (cropz // 2), 0)\n",
    "\n",
    "    # Make sure ending index is <= size\n",
    "    endx = min(startx + cropx, x)\n",
    "    endy = min(starty + cropy, y)\n",
    "    endz = min(startz + cropz, z)\n",
    "\n",
    "    return img[startx:endx, starty:endy, startz:endz, :]\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    \"\"\"\n",
    "    Normalize the pixel values.\n",
    "    This is one of the most important preprocessing steps.\n",
    "    We need to make sure that the pixel values have a mean of 0\n",
    "    and a standard deviation of 1 to help the model to train\n",
    "    faster and more accurately.\n",
    "    \"\"\"\n",
    "\n",
    "    for channel in range(img.shape[3]):\n",
    "        img[:, :, :, channel] = (\n",
    "            img[:, :, :, channel] - np.mean(img[:, :, :, channel])) \\\n",
    "            / np.std(img[:, :, :, channel])\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def attach_attributes(df, json_data, name):\n",
    "    \"\"\"\n",
    "    Save the json data\n",
    "    \"\"\"\n",
    "\n",
    "    if type(json_data) is str:\n",
    "        length = 1\n",
    "    else:\n",
    "        length = len(json_data)\n",
    "\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    dset = df.create_dataset(name, (length,), dtype=dt)\n",
    "    dset[:] = json_data\n",
    "\n",
    "\n",
    "def preprocess_inputs(img):\n",
    "    \"\"\"\n",
    "    Process the input images\n",
    "\n",
    "    For BraTS subset:\n",
    "    INPUT CHANNELS:  \"modality\": {\n",
    "         \"0\": \"FLAIR\", T2-weighted-Fluid-Attenuated Inversion Recovery MRI\n",
    "         \"1\": \"T1w\",  T1-weighted MRI\n",
    "         \"2\": \"t1gd\", T1-gadolinium contrast MRI\n",
    "         \"3\": \"T2w\"   T2-weighted MRI\n",
    "     }\n",
    "    \"\"\"\n",
    "    if len(img.shape) != 4:  # Make sure 4D\n",
    "        img = np.expand_dims(img, -1)\n",
    "\n",
    "    img = crop_center(img, args.resize, args.resize, args.resize)\n",
    "    img = normalize_img(img)\n",
    "\n",
    "    img = np.swapaxes(np.array(img), 0, -2)\n",
    "\n",
    "    # img = img[:,:,:,[0]]  # Just get the FLAIR channel\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_labels(msk):\n",
    "    \"\"\"\n",
    "    Process the ground truth labels\n",
    "\n",
    "    For BraTS subset:\n",
    "    LABEL_CHANNELS: \"labels\": {\n",
    "         \"0\": \"background\",  No tumor\n",
    "         \"1\": \"edema\",       Swelling around tumor\n",
    "         \"2\": \"non-enhancing tumor\",  Tumor that isn't enhanced by Gadolinium contrast\n",
    "         \"3\": \"enhancing tumour\"  Gadolinium contrast enhanced regions\n",
    "     }\n",
    "\n",
    "    \"\"\"\n",
    "    if len(msk.shape) != 4:  # Make sure 4D\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "\n",
    "    msk = crop_center(msk, args.resize, args.resize, args.resize)\n",
    "\n",
    "    # Combining all masks assumes that a mask value of 0 is the background\n",
    "    msk[msk > 1] = 1  # Combine all masks\n",
    "    msk = np.swapaxes(np.array(msk), 0, -2)\n",
    "\n",
    "    return msk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_data_to_hdf5(trainIdx, validateIdx, testIdx, fileIdx,filename, dataDir, json_data):\n",
    "\n",
    "    hdf_file = h5py.File(filename, \"w\")\n",
    "\n",
    "    # Save the dataset attributes\n",
    "    attach_attributes(hdf_file, str(json_data[\"modality\"]), \"modalities\")\n",
    "    attach_attributes(hdf_file, json_data[\"licence\"], \"license\")\n",
    "    attach_attributes(hdf_file, json_data[\"reference\"], \"reference\")\n",
    "    attach_attributes(hdf_file, json_data[\"name\"], \"name\")\n",
    "    attach_attributes(hdf_file, json_data[\"description\"], \"description\")\n",
    "    attach_attributes(hdf_file, json_data[\"release\"], \"release\")\n",
    "    attach_attributes(\n",
    "        hdf_file, json_data[\"tensorImageSize\"], \"tensorImageSize\")\n",
    "\n",
    "    # Training filenames\n",
    "    train_image_files = []\n",
    "    train_label_files = []\n",
    "    for idx in trainIdx:\n",
    "        train_image_files.append(fileIdx[idx][\"image\"])\n",
    "        train_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    # Validation filenames\n",
    "    validate_image_files = []\n",
    "    validate_label_files = []\n",
    "    for idx in validateIdx:\n",
    "        validate_image_files.append(fileIdx[idx][\"image\"])\n",
    "        validate_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    # Testing filenames\n",
    "    test_image_files = []\n",
    "    test_label_files = []\n",
    "    for idx in testIdx:\n",
    "        test_image_files.append(fileIdx[idx][\"image\"])\n",
    "        test_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    attach_attributes(hdf_file, train_image_files, \"training_input_files\")\n",
    "    attach_attributes(hdf_file, train_label_files, \"training_label_files\")\n",
    "    attach_attributes(hdf_file, validate_image_files, \"validation_input_files\")\n",
    "    attach_attributes(hdf_file, validate_label_files, \"validation_label_files\")\n",
    "    attach_attributes(hdf_file, test_image_files, \"testing_input_files\")\n",
    "    attach_attributes(hdf_file, test_label_files, \"testing_label_files\")\n",
    "\n",
    "    \"\"\"\n",
    "    Print shapes of raw data\n",
    "    \"\"\"\n",
    "    print(\"Data shapes\")\n",
    "    print(\"===========\")\n",
    "    print(\"n.b. All tensors converted to stacks of 2D slices.\")\n",
    "    print(\"If you want true 3D tensors, then modify this code appropriately.\")\n",
    "    data_filename = os.path.join(dataDir, train_image_files[0])\n",
    "    img = np.array(nib.load(data_filename).dataobj)\n",
    "    print(\"Raw Image shape     = \", img.shape)\n",
    "    crop_shape = preprocess_inputs(img).shape[1:]\n",
    "    print(\"Cropped Image shape = (?, {}, {}, {})\".format(crop_shape[0],\n",
    "                                                         crop_shape[1],\n",
    "                                                         crop_shape[2]))\n",
    "\n",
    "    data_filename = os.path.join(dataDir, train_label_files[0])\n",
    "    msk = np.array(nib.load(data_filename).dataobj)\n",
    "    print(\"Raw Masks shape     = \", msk.shape)\n",
    "    crop_shape = preprocess_labels(msk).shape[1:]\n",
    "    print(\"Cropped Masks shape = (?, {}, {}, {})\".format(crop_shape[0],\n",
    "                                                         crop_shape[1],\n",
    "                                                         crop_shape[2]))\n",
    "\n",
    "    # Save training set images\n",
    "    print(\"Step 1 of 6. Save training set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(train_image_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "\n",
    "        img = preprocess_inputs(img)\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_train_dset = hdf_file.create_dataset(\"imgs_train\",\n",
    "                                                     img.shape,\n",
    "                                                     maxshape=(None,\n",
    "                                                               img.shape[1],\n",
    "                                                               img.shape[2],\n",
    "                                                               img.shape[3]),\n",
    "                                                     dtype=float,\n",
    "                                                     compression=\"gzip\")\n",
    "            img_train_dset[:] = img\n",
    "        else:\n",
    "            row = img_train_dset.shape[0]  # Count current dataset rows\n",
    "            img_train_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_train_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save validation set images\n",
    "    print(\"Step 2 of 6. Save validation set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(validate_image_files):\n",
    "\n",
    "        # Nibabel should read the file as X,Y,Z,C\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "        img = preprocess_inputs(img)\n",
    "\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_validation_dset = hdf_file.create_dataset(\"imgs_validation\",\n",
    "                                                          img.shape,\n",
    "                                                          maxshape=(None,\n",
    "                                                                    img.shape[1],\n",
    "                                                                    img.shape[2],\n",
    "                                                                    img.shape[3]),\n",
    "                                                          dtype=float,\n",
    "                                                          compression=\"gzip\")\n",
    "            img_validation_dset[:] = img\n",
    "        else:\n",
    "            row = img_validation_dset.shape[0]  # Count current dataset rows\n",
    "            img_validation_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_validation_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save validation set images\n",
    "    print(\"Step 3 of 6. Save testing set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(test_image_files):\n",
    "\n",
    "        # Nibabel should read the file as X,Y,Z,C\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "        img = preprocess_inputs(img)\n",
    "\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_testing_dset = hdf_file.create_dataset(\"imgs_testing\",\n",
    "                                                       img.shape,\n",
    "                                                       maxshape=(None,\n",
    "                                                                 img.shape[1],\n",
    "                                                                 img.shape[2],\n",
    "                                                                 img.shape[3]),\n",
    "                                                       dtype=float,\n",
    "                                                       compression=\"gzip\")\n",
    "            img_testing_dset[:] = img\n",
    "        else:\n",
    "            row = img_testing_dset.shape[0]  # Count current dataset rows\n",
    "            img_testing_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_testing_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save training set masks\n",
    "    print(\"Step 4 of 6. Save training set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(train_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_train_dset = hdf_file.create_dataset(\"msks_train\",\n",
    "                                                     msk.shape,\n",
    "                                                     maxshape=(None,\n",
    "                                                               msk.shape[1],\n",
    "                                                               msk.shape[2],\n",
    "                                                               msk.shape[3]),\n",
    "                                                     dtype=float,\n",
    "                                                     compression=\"gzip\")\n",
    "            msk_train_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_train_dset.shape[0]  # Count current dataset rows\n",
    "            msk_train_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_train_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    # Save testing/validation set masks\n",
    "\n",
    "    print(\"Step 5 of 6. Save validation set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(validate_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_validation_dset = hdf_file.create_dataset(\"msks_validation\",\n",
    "                                                          msk.shape,\n",
    "                                                          maxshape=(None,\n",
    "                                                                    msk.shape[1],\n",
    "                                                                    msk.shape[2],\n",
    "                                                                    msk.shape[3]),\n",
    "                                                          dtype=float,\n",
    "                                                          compression=\"gzip\")\n",
    "            msk_validation_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_validation_dset.shape[0]  # Count current dataset rows\n",
    "            msk_validation_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_validation_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    print(\"Step 6 of 6. Save testing set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(test_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_testing_dset = hdf_file.create_dataset(\"msks_testing\",\n",
    "                                                       msk.shape,\n",
    "                                                       maxshape=(None,\n",
    "                                                                 msk.shape[1],\n",
    "                                                                 msk.shape[2],\n",
    "                                                                 msk.shape[3]),\n",
    "                                                       dtype=float,\n",
    "                                                       compression=\"gzip\")\n",
    "            msk_testing_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_testing_dset.shape[0]  # Count current dataset rows\n",
    "            msk_testing_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_testing_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    hdf_file.close()\n",
    "    print(\"Finished processing.\")\n",
    "    print(\"HDF5 saved to {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "args.save_path, \"{}x{}/\".format(args.resize, args.resize))\n",
    "\n",
    "# Create directory\n",
    "try:\n",
    "    os.makedirs(save_dir)\n",
    "except OSError:\n",
    "    if not os.path.isdir(save_dir):\n",
    "        raise\n",
    "\n",
    "filename = os.path.join(save_dir, args.output_filename)\n",
    "# Check for existing output file and delete if exists\n",
    "if os.path.exists(filename):\n",
    "    print(\"Removing existing data file: {}\".format(filename))\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "==============================\n",
      "Dataset name:         BRATS\n",
      "Dataset description:  Gliomas segmentation tumour and oedema in on brain images\n",
      "Tensor image size:    4D\n",
      "Dataset release:      2.0 04/05/2018\n",
      "Dataset reference:    https://www.med.upenn.edu/sbia/brats2017.html\n",
      "Dataset license:      CC-BY-SA 4.0\n",
      "==============================\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "json_filename = os.path.join(args.data_path, \"dataset.json\")\n",
    "\n",
    "try:\n",
    "    with open(json_filename, \"r\") as fp:\n",
    "        experiment_data = json.load(fp)\n",
    "except IOError as e:\n",
    "    print(\"File {} doesn't exist. It should be part of the \"\" Decathlon directory\".format(json_filename))\n",
    "\n",
    "# Print information about the Decathlon experiment data\n",
    "print(\"*\" * 30)\n",
    "print(\"=\" * 30)\n",
    "print(\"Dataset name:        \", experiment_data[\"name\"])\n",
    "print(\"Dataset description: \", experiment_data[\"description\"])\n",
    "print(\"Tensor image size:   \", experiment_data[\"tensorImageSize\"])\n",
    "print(\"Dataset release:     \", experiment_data[\"release\"])\n",
    "print(\"Dataset reference:   \", experiment_data[\"reference\"])\n",
    "print(\"Dataset license:     \", experiment_data[\"licence\"])  # sic\n",
    "print(\"=\" * 30)\n",
    "print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes\n",
      "===========\n",
      "n.b. All tensors converted to stacks of 2D slices.\n",
      "If you want true 3D tensors, then modify this code appropriately.\n",
      "Raw Image shape     =  (240, 240, 155, 4)\n",
      "Cropped Image shape = (?, 144, 144, 4)\n",
      "Raw Masks shape     = "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/406 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (240, 240, 155)\n",
      "Cropped Masks shape = (?, 144, 144, 1)\n",
      "Step 1 of 6. Save training set images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 406/406 [14:59<00:00,  2.22s/it]\n",
      "  0%|                                                                                           | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 of 6. Save validation set images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [01:12<00:00,  2.28s/it]\n",
      "  0%|                                                                                           | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 of 6. Save testing set images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [01:42<00:00,  2.24s/it]\n",
      "  0%|                                                                                          | 0/406 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 of 6. Save training set masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 406/406 [01:24<00:00,  4.83it/s]\n",
      "  0%|                                                                                           | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 of 6. Save validation set masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 32/32 [00:06<00:00,  4.86it/s]\n",
      "  2%|█▊                                                                                 | 1/46 [00:00<00:09,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 of 6. Save testing set masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 46/46 [00:09<00:00,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing.\n",
      "HDF5 saved to data/decathlon/144x144/Task01_BrainTumour.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed so that always get same random mix\n",
    "np.random.seed(816)\n",
    "numFiles = experiment_data[\"numTraining\"]\n",
    "idxList = np.arange(numFiles)  # List of file indices\n",
    "randomList = np.random.random(numFiles)  # List of random numbers\n",
    "# Random number go from 0 to 1. So anything above\n",
    "# args.train_split is in the validation list.\n",
    "trainList = idxList[randomList < args.split]\n",
    "\n",
    "otherList = idxList[randomList >= args.split]\n",
    "randomList = np.random.random(len(otherList))  # List of random numbers\n",
    "validateList = otherList[randomList >= 0.5]\n",
    "testList = otherList[randomList < 0.5]\n",
    "\n",
    "convert_raw_data_to_hdf5(trainList, validateList, testList,\n",
    "                         experiment_data[\"training\"],\n",
    "                         filename, args.data_path,\n",
    "                         experiment_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
