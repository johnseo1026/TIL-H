{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#웹캠에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) #WIDTH\n",
    "cap.set(4, 480) #HEIGHT\n",
    "\n",
    "#얼굴 인식 캐스케이드 파일 읽는다\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontface.xml')\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    #인식된 얼굴 갯수를 출력\n",
    "    print(len(faces))\n",
    "\n",
    "    # 인식된 얼굴에 사각형을 출력한다\n",
    "    for (x,y,w,h) in faces:\n",
    "         cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "    #화면에 출력한다\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Gender : Female, conf = 0.508\n",
      "Age Output : [[0.00233513 0.0025742  0.00221936 0.00591107 0.05989561 0.08832479\n",
      "  0.74888253 0.08985724]]\n",
      "Age : (48-53), conf = 0.749\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-721213018614>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{},{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgender\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_SIMPLEX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLINE_AA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"age-gender-out.jpg\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;31m#화면에 출력한다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Load network\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "\n",
    "#웹캠에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) #WIDTH\n",
    "cap.set(4, 480) #HEIGHT\n",
    "\n",
    "#얼굴 인식 캐스케이드 파일 읽는다\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontface.xml')\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    #인식된 얼굴 갯수를 출력\n",
    "    print(len(faces))\n",
    "\n",
    "    # 인식된 얼굴에 사각형을 출력한다\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(frame, 1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frame, label, (frame[0], frame[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)        \n",
    "        cv2.imwrite(\"age-gender-out.jpg\", frame)\n",
    "    #화면에 출력한다\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-4-2cb56a9e47e8>, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-2cb56a9e47e8>\"\u001b[1;36m, line \u001b[1;32m79\u001b[0m\n\u001b[1;33m    cv2.destroyAllWindows()\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Load network\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "\n",
    "#웹캠에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) #WIDTH\n",
    "cap.set(4, 480) #HEIGHT\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    net, frame = cap.read()\n",
    "\n",
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn, bboxes\n",
    "\n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)   # 얼굴찾는 함수\n",
    "    padding = 10\n",
    "    for bbox in bboxes:\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)        \n",
    "        cv2.imwrite(\"age-gender-out.jpg\", frameFace)\n",
    "        #화면에 출력한다\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일단 되는 거 바로 아래것 근데 이상함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Gender : Female, conf = 0.789\n",
      "Age Output : [[2.4018640e-04 4.1189529e-03 4.2305097e-01 3.3725653e-02 5.3827155e-01\n",
      "  3.4591169e-04 2.0082122e-04 4.5971079e-05]]\n",
      "Age : (25-32), conf = 0.538\n",
      "1\n",
      "Gender : Male, conf = 0.977\n",
      "Age Output : [[1.7031451e-04 1.1918064e-03 9.5865712e-02 1.9621316e-02 8.8236082e-01\n",
      "  4.4516713e-04 2.5579770e-04 8.9072062e-05]]\n",
      "Age : (25-32), conf = 0.882\n",
      "1\n",
      "Gender : Female, conf = 0.676\n",
      "Age Output : [[4.45920741e-05 8.25899886e-04 4.93968815e-01 8.51017684e-02\n",
      "  4.19576257e-01 3.36460740e-04 1.00863836e-04 4.53994253e-05]]\n",
      "Age : (8-12), conf = 0.494\n",
      "1\n",
      "Gender : Female, conf = 0.618\n",
      "Age Output : [[3.1700658e-04 4.1155727e-03 4.5915049e-01 8.9456014e-02 4.4504473e-01\n",
      "  1.0833145e-03 6.5365573e-04 1.7918802e-04]]\n",
      "Age : (8-12), conf = 0.459\n",
      "1\n",
      "Gender : Male, conf = 0.986\n",
      "Age Output : [[1.7133078e-05 2.2547529e-05 6.0307179e-03 4.7431132e-03 9.8856002e-01\n",
      "  4.3934528e-04 1.3396355e-04 5.3131229e-05]]\n",
      "Age : (25-32), conf = 0.989\n",
      "1\n",
      "Gender : Male, conf = 0.978\n",
      "Age Output : [[1.3964404e-05 2.3172939e-05 5.1881815e-03 4.5714309e-03 9.8960680e-01\n",
      "  4.3881274e-04 1.1101517e-04 4.6602876e-05]]\n",
      "Age : (25-32), conf = 0.990\n",
      "1\n",
      "Gender : Male, conf = 0.797\n",
      "Age Output : [[1.1616872e-04 2.7258690e-03 5.0252992e-01 1.4611894e-01 3.4709123e-01\n",
      "  8.8197622e-04 4.1484248e-04 1.2105179e-04]]\n",
      "Age : (8-12), conf = 0.503\n",
      "1\n",
      "Gender : Male, conf = 0.643\n",
      "Age Output : [[4.3099510e-05 9.1375894e-04 5.5822277e-01 6.3806854e-02 3.7613726e-01\n",
      "  4.6067868e-04 3.3946752e-04 7.6146607e-05]]\n",
      "Age : (8-12), conf = 0.558\n",
      "1\n",
      "Gender : Male, conf = 0.904\n",
      "Age Output : [[2.5587509e-05 4.4421907e-04 3.9290395e-01 4.4817075e-02 5.6101114e-01\n",
      "  4.7327115e-04 2.6710040e-04 5.7639238e-05]]\n",
      "Age : (25-32), conf = 0.561\n",
      "1\n",
      "Gender : Male, conf = 0.991\n",
      "Age Output : [[4.3931876e-05 2.1529327e-04 3.1974103e-02 7.4173687e-03 9.5989263e-01\n",
      "  2.7773977e-04 1.3269122e-04 4.6269532e-05]]\n",
      "Age : (25-32), conf = 0.960\n",
      "1\n",
      "Gender : Male, conf = 0.747\n",
      "Age Output : [[1.2985709e-04 1.9827166e-03 3.0897659e-01 2.1024926e-02 6.6575283e-01\n",
      "  1.0139452e-03 9.8079594e-04 1.3838171e-04]]\n",
      "Age : (25-32), conf = 0.666\n",
      "1\n",
      "Gender : Female, conf = 0.942\n",
      "Age Output : [[1.3325106e-04 1.5524043e-03 2.9135147e-01 3.5128564e-02 6.7094541e-01\n",
      "  6.2407943e-04 1.7821169e-04 8.6608692e-05]]\n",
      "Age : (25-32), conf = 0.671\n",
      "1\n",
      "Gender : Male, conf = 0.537\n",
      "Age Output : [[2.1875362e-04 1.6879557e-03 9.6472941e-02 2.3629088e-02 8.7661868e-01\n",
      "  1.0487310e-03 2.3259567e-04 9.1130612e-05]]\n",
      "Age : (25-32), conf = 0.877\n",
      "1\n",
      "Gender : Female, conf = 0.692\n",
      "Age Output : [[9.9167301e-05 1.1266115e-03 1.4895412e-01 3.9282504e-02 8.0928427e-01\n",
      "  1.0060915e-03 1.7689106e-04 7.0251968e-05]]\n",
      "Age : (25-32), conf = 0.809\n",
      "1\n",
      "Gender : Female, conf = 0.618\n",
      "Age Output : [[7.6135312e-04 5.4355627e-03 1.1784328e-01 2.9138150e-02 8.4411466e-01\n",
      "  1.8788062e-03 6.0813129e-04 2.2006073e-04]]\n",
      "Age : (25-32), conf = 0.844\n",
      "1\n",
      "Gender : Female, conf = 0.543\n",
      "Age Output : [[2.8118503e-04 2.9703421e-03 2.1744224e-01 3.1266801e-02 7.4668342e-01\n",
      "  1.0825484e-03 1.8583627e-04 8.7614477e-05]]\n",
      "Age : (25-32), conf = 0.747\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn, bboxes\n",
    "\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Load network\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "\n",
    "#웹캠에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) #WIDTH\n",
    "cap.set(4, 480) #HEIGHT\n",
    "\n",
    "#얼굴 인식 캐스케이드 파일 읽는다\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontface.xml')\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    #인식된 얼굴 갯수를 출력\n",
    "    print(len(faces))\n",
    "\n",
    "    # 인식된 얼굴에 사각형을 출력한다\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "            \n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)   # 얼굴찾는 함수\n",
    "    padding = 10\n",
    "    for bbox in bboxes:\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)        \n",
    "        cv2.imwrite(\"age-gender-out.jpg\", frameFace)\n",
    "    #화면에 출력한다\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신호가 들어올때 검출하는 저장키 같은것 만들자\n",
    "아니면 시간이 어느정도 지낫을때의 조건을 줘서 저장하게\n",
    "\n",
    "\n",
    "frame에 또 frame이 들어가서 2개가 생김"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장 키값 추가 해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Gender : Female, conf = 0.996\n",
      "Age Output : [[3.0921998e-03 3.4345996e-03 2.3918611e-01 4.6070036e-03 7.4688077e-01\n",
      "  1.8907061e-03 7.1117096e-04 1.9732445e-04]]\n",
      "Age : (25-32), conf = 0.747\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9756ef2fe728>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m#화면에 출력한다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'frame'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m26\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'key' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn, bboxes\n",
    "\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Load network\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "\n",
    "#웹캠에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) #WIDTH\n",
    "cap.set(4, 480) #HEIGHT\n",
    "\n",
    "#얼굴 인식 캐스케이드 파일 읽는다\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontface.xml')\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    #인식된 얼굴 갯수를 출력\n",
    "    print(len(faces))\n",
    "\n",
    "    # 인식된 얼굴에 사각형을 출력한다\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "            \n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)   # 얼굴찾는 함수\n",
    "    padding = 10\n",
    "    for bbox in bboxes:\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)        \n",
    "        cv2.imwrite(\"age-gender-out.jpg\", frameFace)\n",
    "    #화면에 출력한다\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to operator (<ipython-input-25-28854be14046>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-28854be14046>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    x+w = int(detections[0, 0, i, 5] * frameWidth)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m can't assign to operator\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv2\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def getFaceBox(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    bboxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x+w = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y+h = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            bboxes.append([x, y, x+w, y+h])\n",
    "            cv2.rectangle(frameOpencvDnn,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    return frameOpencvDnn, bboxes\n",
    "\n",
    "\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "# Load network\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "\n",
    "\n",
    "#웹캠에서 영상을 읽어온다\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, 640) #WIDTH\n",
    "cap.set(4, 480) #HEIGHT\n",
    "\n",
    "#얼굴 인식 캐스케이드 파일 읽는다\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontface.xml')\n",
    "\n",
    "while(True):\n",
    "    # frame 별로 capture 한다\n",
    "    net, frame = cap.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    #인식된 얼굴 갯수를 출력\n",
    "    print(len(faces))\n",
    "\n",
    "    # 인식된 얼굴에 사각형을 출력한다\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "\n",
    "            \n",
    "    frameFace, bboxes = getFaceBox(faceNet, frame)   # 얼굴찾는 함수\n",
    "    padding = 10\n",
    "    for bbox in bboxes:\n",
    "        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227),(78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(\"Gender : {}, conf = {:.3f}\".format(gender, genderPreds[0].max()))\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(\"Age Output : {}\".format(agePreds))\n",
    "        print(\"Age : {}, conf = {:.3f}\".format(age, agePreds[0].max()))\n",
    "\n",
    "        label = \"{},{}\".format(gender, age)\n",
    "        cv2.putText(frameFace, label, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)        \n",
    "        cv2.imwrite(\"age-gender-out.jpg\", frameFace)\n",
    "    #화면에 출력한다\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "for i in range(10):\n",
    "    return_value, image = camera.read()\n",
    "    cv2.imwrite('opencv'+str(i)+'.png', image)\n",
    "del(camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opencv_frame_0.png written!\n",
      "Escape hit, closing...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "cv2.namedWindow(\"test\")\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    cv2.imshow(\"test\", frame)\n",
    "    if not ret:\n",
    "        break\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "    elif k%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        img_counter += 1\n",
    "\n",
    "cam.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
