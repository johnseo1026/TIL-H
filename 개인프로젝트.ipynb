{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹캠에서 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "\n",
    "enableGenderIcons = True\n",
    "\n",
    "male_icon = cv2.imread(\"male.jpg\")\n",
    "male_icon = cv2.resize(male_icon, (40, 40))\n",
    "\n",
    "female_icon = cv2.imread(\"female.jpg\")\n",
    "female_icon = cv2.resize(female_icon, (40, 40))\n",
    "#-----------------------\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def loadVggFaceModel():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(ZeroPadding2D((1,1)))\n",
    "\tmodel.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "\tmodel.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Convolution2D(2622, (1, 1)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Activation('softmax'))\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "def ageModel():\n",
    "\tmodel = loadVggFaceModel()\n",
    "\t\n",
    "\tbase_model_output = Sequential()\n",
    "\tbase_model_output = Convolution2D(101, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "\tbase_model_output = Flatten()(base_model_output)\n",
    "\tbase_model_output = Activation('softmax')(base_model_output)\n",
    "\t\n",
    "\tage_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\t\n",
    "\t#you can find the pre-trained weights for age prediction here: https://drive.google.com/file/d/1YCox_4kJ-BYeXq27uUbasu--yz28zUMV/view?usp=sharing\n",
    "\tage_model.load_weights(\"age_model_weights.h5\")\n",
    "\t\n",
    "\treturn age_model\n",
    "\n",
    "def genderModel():\n",
    "\tmodel = loadVggFaceModel()\n",
    "\t\n",
    "\tbase_model_output = Sequential()\n",
    "\tbase_model_output = Convolution2D(2, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "\tbase_model_output = Flatten()(base_model_output)\n",
    "\tbase_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "\tgender_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\t\n",
    "\t#you can find the pre-trained weights for gender prediction here: https://drive.google.com/file/d/1wUXRVlbsni2FN9-jkS_f4UTUrm1bRLyk/view?usp=sharing\n",
    "\tgender_model.load_weights(\"gender_model_weights.h5\")\n",
    "\t\n",
    "\treturn gender_model\n",
    "\t\n",
    "age_model = ageModel()\n",
    "gender_model = genderModel()\n",
    "\n",
    "#age model has 101 outputs and its outputs will be multiplied by its index label. sum will be apparent age\n",
    "output_indexes = np.array([i for i in range(0, 101)])\n",
    "\n",
    "#------------------------\n",
    "\n",
    "cap = cv2.VideoCapture(0) #capture webcam\n",
    "\n",
    "while(True):\n",
    "\tret, img = cap.read()\n",
    "\t#img = cv2.resize(img, (640, 360))\n",
    "\t\n",
    "\tfaces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\t\n",
    "\tfor (x,y,w,h) in faces:\n",
    "\t\tif w > 130: #ignore small faces\n",
    "\t\t\t\n",
    "\t\t\t#mention detected face\n",
    "\t\t\t\"\"\"overlay = img.copy(); output = img.copy(); opacity = 0.6\n",
    "\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),cv2.FILLED) #draw rectangle to main image\n",
    "\t\t\tcv2.addWeighted(overlay, opacity, img, 1 - opacity, 0, img)\"\"\"\n",
    "\t\t\tcv2.rectangle(img,(x,y),(x+w,y+h),(128,128,128),1) #draw rectangle to main image\n",
    "\t\t\t\n",
    "\t\t\t#extract detected face\n",
    "\t\t\tdetected_face = img[int(y):int(y+h), int(x):int(x+w)] #crop detected face\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#age gender data set has 40% margin around the face. expand detected face.\n",
    "\t\t\t\tmargin = 30\n",
    "\t\t\t\tmargin_x = int((w * margin)/100); margin_y = int((h * margin)/100)\n",
    "\t\t\t\tdetected_face = img[int(y-margin_y):int(y+h+margin_y), int(x-margin_x):int(x+w+margin_x)]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tprint(\"detected face has no margin\")\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\t#vgg-face expects inputs (224, 224, 3)\n",
    "\t\t\t\tdetected_face = cv2.resize(detected_face, (224, 224))\n",
    "\t\t\t\t\n",
    "\t\t\t\timg_pixels = image.img_to_array(detected_face)\n",
    "\t\t\t\timg_pixels = np.expand_dims(img_pixels, axis = 0)\n",
    "\t\t\t\timg_pixels /= 255\n",
    "\t\t\t\t\n",
    "\t\t\t\t#find out age and gender\n",
    "\t\t\t\tage_distributions = age_model.predict(img_pixels)\n",
    "\t\t\t\tapparent_age = str(int(np.floor(np.sum(age_distributions * output_indexes, axis = 1))[0]))\n",
    "\t\t\t\t\n",
    "\t\t\t\tgender_distribution = gender_model.predict(img_pixels)[0]\n",
    "\t\t\t\tgender_index = np.argmax(gender_distribution)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif gender_index == 0: gender = \"F\"\n",
    "\t\t\t\telse: gender = \"M\"\n",
    "\t\t\t\n",
    "\t\t\t\t#background for age gender declaration\n",
    "\t\t\t\tinfo_box_color = (46,200,255)\n",
    "\t\t\t\t#triangle_cnt = np.array( [(x+int(w/2), y+10), (x+int(w/2)-25, y-20), (x+int(w/2)+25, y-20)] )\n",
    "\t\t\t\ttriangle_cnt = np.array( [(x+int(w/2), y), (x+int(w/2)-20, y-20), (x+int(w/2)+20, y-20)] )\n",
    "\t\t\t\tcv2.drawContours(img, [triangle_cnt], 0, info_box_color, -1)\n",
    "\t\t\t\tcv2.rectangle(img,(x+int(w/2)-50,y-20),(x+int(w/2)+50,y-90),info_box_color,cv2.FILLED)\n",
    "\t\t\t\t\n",
    "\t\t\t\t#labels for age and gender\n",
    "\t\t\t\tcv2.putText(img, apparent_age, (x+int(w/2), y - 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
    "\t\t\t\t\n",
    "\t\t\t\tif enableGenderIcons:\n",
    "\t\t\t\t\tif gender == 'M': gender_icon = male_icon\n",
    "\t\t\t\t\telse: gender_icon = female_icon\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\timg[y-75:y-75+male_icon.shape[0], x+int(w/2)-45:x+int(w/2)-45+male_icon.shape[1]] = gender_icon\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcv2.putText(img, gender, (x+int(w/2)-42, y - 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 111, 255), 2)\n",
    "\t\t\t\t\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(\"exception\",str(e))\n",
    "\t\t\t\n",
    "\tcv2.imshow('img',img)\n",
    "\t\n",
    "\tif cv2.waitKey(1) & 0xFF == ord('q'): #press q to quit\n",
    "\t\tbreak\n",
    "\t\n",
    "#kill open cv things\t\t\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
